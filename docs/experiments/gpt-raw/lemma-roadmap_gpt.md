# Proof-oriented lemma candidates (GPT 5.2 Pro extract)

Source: ChatGPT response extracted via `~/clawd/scripts/chatgpt-extract.sh` (see `/tmp/lemma_candidates.txt`).

---

Hereâ€™s a proofâ€‘oriented roadmap that takes your M2â€“M4 measurements and tries to compress them into a small set of lemma-shaped targetsâ€”each one something a human could plausibly prove with finiteâ€‘state methods, explicit constants, and controlled exceptional sets.

 

One metaâ€‘remark up front: what youâ€™re doing (modâ€‘
3
ğ‘˜
3
k
 mixing + drift control) is very much in the same â€œshapeâ€ as the hard technical ingredient in Terence Taoâ€™s â€œalmost allâ€ result, where a key step is bounding characteristic functions of a skew random walk on a 
3
3-adic cyclic group. 
arXiv
 That doesnâ€™t give you a proof for freeâ€”but itâ€™s a strong hint that your chosen observables (M2â€“M4) are aimed at real proof bottlenecks, not decorative statistics.

 

Below, Iâ€™ll write the candidates as â€œlemmasâ€, but you should read them as proof targets (some are conditional on making the â€œevolved measureâ€ precise).

Lemma candidate 1 â€” Finiteâ€‘state Markov reduction and spectral gap on 
â€Š
m
o
d
â€Š
3
ğ‘˜
mod3
k
1) Precise statement (conditional but crisp)

Fix 
ğ‘˜
â‰¥
1
kâ‰¥1. Let 
ğ‘†
ğ‘˜
:
=
(
ğ‘
/
3
ğ‘˜
ğ‘
)
Ã—
S
k
	â€‹

:=(Z/3
k
Z)
Ã—
 (the units; equivalently residue classes not divisible by 
3
3).
For an odd starting integer 
ğ‘›
0
n
0
	â€‹

, define the Syracuse trajectory 
ğ‘›
ğ‘¡
+
1
=
ğ‘‡
(
ğ‘›
ğ‘¡
)
n
t+1
	â€‹

=T(n
t
	â€‹

) and the state process

ğ‘‹
ğ‘¡
:
=
ğ‘›
ğ‘¡
(
m
o
d
3
ğ‘˜
)
âˆˆ
ğ‘†
ğ‘˜
(
ğ‘¡
â‰¥
1
)
.
X
t
	â€‹

:=n
t
	â€‹

(mod3
k
)âˆˆS
k
	â€‹

(tâ‰¥1).

(As you know, 
3
âˆ¤
ğ‘›
ğ‘¡
3âˆ¤n
t
	â€‹

 for all 
ğ‘¡
â‰¥
1
tâ‰¥1, so the state really does live in 
ğ‘†
ğ‘˜
S
k
	â€‹

.)

 

Lemma target (Markovization + mixing). There exists a timeâ€‘homogeneous Markov kernel 
ğ‘ƒ
ğ‘˜
P
k
	â€‹

 on 
ğ‘†
ğ‘˜
S
k
	â€‹

, a stationary distribution 
ğœ‹
ğ‘˜
Ï€
k
	â€‹

, and constants 
ğ¶
ğ‘˜
<
âˆ
C
k
	â€‹

<âˆ, 
0
<
ğœŒ
ğ‘˜
<
1
0<Ï
k
	â€‹

<1 such that the following holds for the â€œevolved measureâ€ 
ğœ‡
Î¼ you are using:

If 
ğ‘›
0
âˆ¼
ğœ‡
n
0
	â€‹

âˆ¼Î¼ and 
ğ‘‹
ğ‘¡
X
t
	â€‹

 is defined as above, then for all 
ğ‘¡
â‰¥
0
tâ‰¥0,

âˆ¥
ğ¿
(
ğ‘‹
ğ‘¡
)
âˆ’
ğœ‹
ğ‘˜
âˆ¥
T
V
â‰¤
ğ¶
ğ‘˜
â€‰
ğœŒ
ğ‘˜
â€‰
ğ‘¡
.
	â€‹

L(X
t
	â€‹

)âˆ’Ï€
k
	â€‹

	â€‹

TV
	â€‹

â‰¤C
k
	â€‹

Ï
k
t
	â€‹

.

Moreover, the joint law of 
(
ğ‘‹
ğ‘¡
,
ğ‘‹
ğ‘¡
+
1
)
(X
t
	â€‹

,X
t+1
	â€‹

) is exactly (or up to a quantified 
ğœ€
ğ‘˜
Îµ
k
	â€‹

) given by 
ğœ‹
ğ‘˜
(
ğ‘¥
)
ğ‘ƒ
ğ‘˜
(
ğ‘¥
,
ğ‘¦
)
Ï€
k
	â€‹

(x)P
k
	â€‹

(x,y).

A slightly weaker (often easier) version is: there is a spectral gap,

ğœ†
2
(
ğ‘ƒ
ğ‘˜
)
â‰¤
1
âˆ’
ğ›¾
ğ‘˜
forÂ someÂ 
ğ›¾
ğ‘˜
>
0
,
Î»
2
	â€‹

(P
k
	â€‹

)â‰¤1âˆ’Î³
k
	â€‹

forÂ someÂ Î³
k
	â€‹

>0,

in the 
â„“
2
(
ğœ‹
ğ‘˜
)
â„“
2
(Ï€
k
	â€‹

) sense.

2) Why this matters

This turns your infinite deterministic system into a finite, analyzable object: an ergodic chain on 
ğ‘†
ğ‘˜
S
k
	â€‹

. Once you have a spectral gap, you get:

exponential decay of correlations for any bounded observable of 
ğ‘‹
ğ‘¡
X
t
	â€‹

;

the machinery you need for Poisson equations, additive functionals, large deviations, and Fosterâ€“Lyapunov criteria.

In other words: this lemma is the bridge from â€œwe saw mixingâ€ to â€œwe can actually prove drift/recurrence statements with constants.â€

3) What M2 evidence supports it

Strong support looks like:

Empirical transition matrix 
ğ‘ƒ
^
ğ‘˜
P
k
	â€‹

 (estimated from long runs under your evolved measure) has:

a clear stationary vector 
ğœ‹
^
ğ‘˜
Ï€
k
	â€‹

 (often close to uniform on 
ğ‘†
ğ‘˜
S
k
	â€‹

);

a stable estimate of the second eigenvalue magnitude 
âˆ£
ğœ†
^
2
âˆ£
âˆ£
Î»
^
2
	â€‹

âˆ£ bounded away from 1;

Total variation (or chiâ€‘square) distance of 
ğ‘‹
ğ‘¡
X
t
	â€‹

 from 
ğœ‹
^
ğ‘˜
Ï€
k
	â€‹

 decays roughly exponentially in 
ğ‘¡
t, and the rate is robust across:

different initial distributions (uniform in an interval vs logâ€‘weighted vs conditioned on size),

different magnitude bands of 
ğ‘›
n.

4) Plausible proof strategy

Two humanâ€‘scale routes:

 

(A) Fourier/spectral route on 
ğ‘
/
3
ğ‘˜
ğ‘
Z/3
k
Z
Prove decay of Fourier coefficients of the pushforward distribution:

âˆ£
ğ¸
â€‰
ğ‘’
2
ğœ‹
ğ‘–
ğœ‰
ğ‘‹
ğ‘¡
/
3
ğ‘˜
âˆ£
â‰¤
ğ¶
â€‰
ğœŒ
ğ‘¡
(
ğœ‰
â‰ 
0
)
.
	â€‹

Ee
2Ï€iÎ¾X
t
	â€‹

/3
k
	â€‹

â‰¤CÏ
t
(Î¾
î€ 
=0).

This is exactly the kind of â€œhigh frequencyâ€ control that appears in Taoâ€™s approach to Syracuse. 
arXiv

Your empirical M2 can tell you which frequencies are the last to die, i.e., where the proof effort needs to go.

 

(B) Doeblin/minorization route (finiteâ€‘step smoothing)
Try to prove there exists 
ğ‘š
=
ğ‘š
(
ğ‘˜
)
m=m(k) and 
ğœ‚
>
0
Î·>0 such that

ğ‘ƒ
ğ‘˜
ğ‘š
(
ğ‘¥
,
â‹…
)
â‰¥
ğœ‚
â€‰
ğœˆ
(
â‹…
)
âˆ€
ğ‘¥
âˆˆ
ğ‘†
ğ‘˜
P
k
m
	â€‹

(x,â‹…)â‰¥Î·Î½(â‹…)âˆ€xâˆˆS
k
	â€‹


for some reference measure 
ğœˆ
Î½ on 
ğ‘†
ğ‘˜
S
k
	â€‹

. That implies geometric ergodicity. This often reduces to showing that within 
ğ‘š
m steps, the random â€œdivision by 
2
ğ‘
2
a
â€ component can generate enough spread in the multiplicative group.

5) Minimal extra experiments to deâ€‘risk it

Estimate 
ğœ†
^
2
Î»
^
2
	â€‹

 (and maybe a few next eigenvalues) of 
ğ‘ƒ
^
ğ‘˜
P
k
	â€‹

 for 
ğ‘˜
=
6
,
8
,
10
,
12
k=6,8,10,12. Look for a gap that doesnâ€™t collapse with 
ğ‘˜
k.

Fourier diagnostic: track 
ğœ™
^
ğ‘¡
(
ğœ‰
)
:
=
ğ¸
(
ğ‘’
2
ğœ‹
ğ‘–
ğœ‰
ğ‘‹
ğ‘¡
/
3
ğ‘˜
)
Ï•
	â€‹

t
	â€‹

(Î¾):=E(e
2Ï€iÎ¾X
t
	â€‹

/3
k
) over 
ğ‘¡
t, and identify the slowestâ€‘decaying 
ğœ‰
Î¾. Those 
ğœ‰
Î¾ are your proof targets.

Stress test with multiple initial measures (uniform vs logâ€‘uniform vs conditioned on large 
ğ‘›
n).

Lemma candidate 2 â€” Conditional geometric law for 
ğ‘
(
ğ‘›
)
a(n) and short memory
1) Precise statement

Let 
ğ‘
ğ‘¡
:
=
ğ‘
(
ğ‘›
ğ‘¡
)
=
ğ‘£
2
(
3
ğ‘›
ğ‘¡
+
1
)
a
t
	â€‹

:=a(n
t
	â€‹

)=v
2
	â€‹

(3n
t
	â€‹

+1). Fix 
ğ‘˜
k and view the joint process 
(
ğ‘‹
ğ‘¡
,
ğ‘
ğ‘¡
)
(X
t
	â€‹

,a
t
	â€‹

).

 

Lemma target (conditional law + mixing). Under the evolved/stationary regime,
there exist 
ğœ€
ğ‘˜
â†’
0
Îµ
k
	â€‹

â†’0 and 
0
<
ğœƒ
ğ‘˜
<
1
0<Î¸
k
	â€‹

<1 such that:

(Uniform conditional geometric tail)

sup
â¡
ğ‘¥
âˆˆ
ğ‘†
ğ‘˜
Â 
sup
â¡
ğ‘š
â‰¥
1
Â 
âˆ£
ğ‘ƒ
(
ğ‘
ğ‘¡
=
ğ‘š
âˆ£
ğ‘‹
ğ‘¡
=
ğ‘¥
)
âˆ’
2
âˆ’
ğ‘š
âˆ£
Â 
â‰¤
Â 
ğœ€
ğ‘˜
â€‰
2
âˆ’
ğ‘š
.
xâˆˆS
k
	â€‹

sup
	â€‹

Â 
mâ‰¥1
sup
	â€‹

Â 
	â€‹

P(a
t
	â€‹

=mâˆ£X
t
	â€‹

=x)âˆ’2
âˆ’m
	â€‹

Â â‰¤Â Îµ
k
	â€‹

2
âˆ’m
.

(Exponential decorrelation)
For bounded functions 
ğ¹
,
ğº
F,G,

âˆ£
C
o
v
(
ğ¹
(
ğ‘
ğ‘¡
,
ğ‘‹
ğ‘¡
)
,
â€‰
ğº
(
ğ‘
ğ‘¡
+
ğ‘ 
,
ğ‘‹
ğ‘¡
+
ğ‘ 
)
)
âˆ£
â‰¤
ğ¶
â€‰
ğœƒ
ğ‘˜
â€‰
ğ‘ 
â€‰
âˆ¥
ğ¹
âˆ¥
âˆ
âˆ¥
ğº
âˆ¥
âˆ
.
âˆ£Cov(F(a
t
	â€‹

,X
t
	â€‹

),G(a
t+s
	â€‹

,X
t+s
	â€‹

))âˆ£â‰¤CÎ¸
k
s
	â€‹

âˆ¥Fâˆ¥
âˆ
	â€‹

âˆ¥Gâˆ¥
âˆ
	â€‹

.

(You can weaken â€œuniform in 
ğ‘¥
xâ€ to â€œ
ğœ‹
ğ‘˜
Ï€
k
	â€‹

-a.e. 
ğ‘¥
xâ€ if needed.)

2) Why this matters

This is the â€œnoise model becomes realâ€ lemma.

The heuristic negative drift comes from 
ğ‘
a behaving roughly geometric with mean 2, giving average logâ€‘drift 
â‰ˆ
log
â¡
3
âˆ’
2
log
â¡
2
=
log
â¡
(
3
/
4
)
<
0
â‰ˆlog3âˆ’2log2=log(3/4)<0.

To convert that into a proof, you need not just the marginal distribution, but control of correlations (otherwise rare long correlated stretches of small 
ğ‘
ğ‘¡
a
t
	â€‹

 can dominate).

If you can prove this lemma (even with modest 
ğœ€
ğ‘˜
Îµ
k
	â€‹

), youâ€™re 80% of the way to largeâ€‘deviation bounds for block drift (Lemma 4).

3) What M3 evidence supports it

The histogram of 
ğ‘
ğ‘¡
a
t
	â€‹

 under the evolved measure matches 
2
âˆ’
ğ‘š
2
âˆ’m
 over a decent range of 
ğ‘š
m (say 
ğ‘š
â‰¤
15
mâ‰¤15 before sample noise dominates).

Conditional histograms given 
ğ‘‹
ğ‘¡
=
ğ‘¥
X
t
	â€‹

=x (or coarse bins of 
ğ‘¥
x) remain close to geometricâ€”no â€œtoxic residue classesâ€ where 
ğ‘
=
1
,
2
a=1,2 are anomalously frequent.

Autocorrelation of 
ğ‘
ğ‘¡
a
t
	â€‹

 (and of indicators 
1
ğ‘
ğ‘¡
=
ğ‘š
1
a
t
	â€‹

=m
	â€‹

) drops to near 0 quickly; mutual information 
ğ¼
(
ğ‘
ğ‘¡
;
ğ‘
ğ‘¡
+
ğ‘ 
)
I(a
t
	â€‹

;a
t+s
	â€‹

) decays roughly exponentially.

4) Plausible proof strategy

(A) Reduce to correlation bounds for congruence constraints.
Events like 
{
ğ‘
ğ‘¡
â‰¥
ğ‘š
}
{a
t
	â€‹

â‰¥m} are congruence conditions mod 
2
ğ‘š
2
m
 on 
ğ‘›
ğ‘¡
n
t
	â€‹

. Track how these congruence classes pull back through 
ğ‘‡
T. For fixed finite patterns 
(
ğ‘
ğ‘¡
,
â€¦
,
ğ‘
ğ‘¡
+
â„“
)
(a
t
	â€‹

,â€¦,a
t+â„“
	â€‹

), you can often write a single affine congruence constraint modulo 
2
âˆ‘
ğ‘
ğ‘–
2
âˆ‘a
i
	â€‹

 (plus oddness constraints). Then the problem becomes a counting / equidistribution estimate.

 

(B) Use Lemma 1â€™s spectral gap: additive functional of a finite chain.
If 
(
ğ‘‹
ğ‘¡
)
(X
t
	â€‹

) mixes geometrically and the conditional law of 
ğ‘
ğ‘¡
a
t
	â€‹

 given 
ğ‘‹
ğ‘¡
X
t
	â€‹

 is controlled, then 
(
ğ‘‹
ğ‘¡
,
ğ‘
ğ‘¡
)
(X
t
	â€‹

,a
t
	â€‹

) is a hidden Markov model with exponential mixing. Standard finiteâ€‘state techniques give decorrelation.

5) Minimal extra experiments to deâ€‘risk it

Conditional KL divergence: 
ğ·
K
L
(
ğ‘ƒ
^
(
ğ‘
âˆ£
ğ‘‹
=
ğ‘¥
)
â€‰
âˆ¥
â€‰
Geom
(
1
/
2
)
)
D
KL
	â€‹

(
P
(aâˆ£X=x)âˆ¥Geom(1/2)) as a function of 
ğ‘¥
x, then summarize by quantiles over 
ğ‘¥
x. You want â€œno fat right tailâ€ in that divergence.

Autocorrelation/mutual information not just for 
ğ‘
ğ‘¡
a
t
	â€‹

, but for the drift increment 
ğ‘‘
ğ‘¡
:
=
log
â¡
(
3
+
1
/
ğ‘›
ğ‘¡
)
âˆ’
ğ‘
ğ‘¡
log
â¡
2
d
t
	â€‹

:=log(3+1/n
t
	â€‹

)âˆ’a
t
	â€‹

log2.

Check stability across magnitude bins of 
ğ‘›
ğ‘¡
n
t
	â€‹

. If the law depends on 
log
â¡
ğ‘›
logn, you may need to include a coarse â€œsize binâ€ into the state.

Lemma candidate 3 â€” Poisson correction 
ğœ“
ğ‘˜
Ïˆ
k
	â€‹

 and a Fosterâ€“Lyapunov drift inequality

This is the â€œturn M4 into an actual Lyapunov functionâ€ move.

1) Precise statement

Define the oneâ€‘step log increment

Î”
ğ‘¡
:
=
log
â¡
ğ‘›
ğ‘¡
+
1
âˆ’
log
â¡
ğ‘›
ğ‘¡
=
log
â¡
(
3
+
1
/
ğ‘›
ğ‘¡
)
âˆ’
ğ‘
ğ‘¡
log
â¡
2.
Î”
t
	â€‹

:=logn
t+1
	â€‹

âˆ’logn
t
	â€‹

=log(3+1/n
t
	â€‹

)âˆ’a
t
	â€‹

log2.

Fix 
ğ‘˜
k and let 
ğ‘‹
ğ‘¡
=
ğ‘›
ğ‘¡
â€Š
m
o
d
â€Š
3
ğ‘˜
X
t
	â€‹

=n
t
	â€‹

mod3
k
. Suppose Lemma 1 gives a Markov kernel 
ğ‘ƒ
ğ‘˜
P
k
	â€‹

 and stationary 
ğœ‹
ğ‘˜
Ï€
k
	â€‹

. Define the stateâ€‘dependent mean increment (under the stationary regime)

ğ‘”
ğ‘˜
(
ğ‘¥
)
:
=
ğ¸
[
Î”
ğ‘¡
âˆ£
ğ‘‹
ğ‘¡
=
ğ‘¥
]
.
g
k
	â€‹

(x):=E[Î”
t
	â€‹

âˆ£X
t
	â€‹

=x].

Let 
ğ‘
ğ‘˜
:
=
ğ¸
ğœ‹
ğ‘˜
[
ğ‘”
ğ‘˜
(
ğ‘‹
)
]
c
k
	â€‹

:=E
Ï€
k
	â€‹

	â€‹

[g
k
	â€‹

(X)] be the stationary average drift.

 

Lemma target (Poisson/Fosterâ€“Lyapunov).
There exists a bounded function 
ğœ“
ğ‘˜
:
ğ‘†
ğ‘˜
â†’
ğ‘…
Ïˆ
k
	â€‹

:S
k
	â€‹

â†’R solving the Poisson equation

(
ğ¼
âˆ’
ğ‘ƒ
ğ‘˜
)
ğœ“
ğ‘˜
=
ğ‘”
ğ‘˜
âˆ’
ğ‘
ğ‘˜
,
(Iâˆ’P
k
	â€‹

)Ïˆ
k
	â€‹

=g
k
	â€‹

âˆ’c
k
	â€‹

,

and constants 
ğ›¿
>
0
Î´>0, 
ğ‘
0
N
0
	â€‹

 such that if 
ğ‘
ğ‘˜
â‰¤
âˆ’
2
ğ›¿
c
k
	â€‹

â‰¤âˆ’2Î´, then for all 
ğ‘›
â‰¥
ğ‘
0
nâ‰¥N
0
	â€‹

,

ğ¸
[
(
log
â¡
ğ‘›
ğ‘¡
+
1
+
ğœ“
ğ‘˜
(
ğ‘‹
ğ‘¡
+
1
)
)
âˆ’
(
log
â¡
ğ‘›
ğ‘¡
+
ğœ“
ğ‘˜
(
ğ‘‹
ğ‘¡
)
)
Â 
âˆ£
Â 
ğ‘›
ğ‘¡
=
ğ‘›
]
â‰¤
âˆ’
ğ›¿
.
E[(logn
t+1
	â€‹

+Ïˆ
k
	â€‹

(X
t+1
	â€‹

))âˆ’(logn
t
	â€‹

+Ïˆ
k
	â€‹

(X
t
	â€‹

))Â 
	â€‹

Â n
t
	â€‹

=n]â‰¤âˆ’Î´.

Equivalently, 
ğ‘‰
ğ‘˜
(
ğ‘›
)
:
=
log
â¡
ğ‘›
+
ğœ“
ğ‘˜
(
ğ‘›
â€Š
m
o
d
â€Š
3
ğ‘˜
)
V
k
	â€‹

(n):=logn+Ïˆ
k
	â€‹

(nmod3
k
) has strictly negative drift above 
ğ‘
0
N
0
	â€‹

.

2) Why this matters

This is the classic â€œrandom walk with stateâ€‘dependent driftâ€ fix:

Raw drift 
ğ¸
[
Î”
ğ‘¡
âˆ£
ğ‘‹
ğ‘¡
=
ğ‘¥
]
E[Î”
t
	â€‹

âˆ£X
t
	â€‹

=x] might vary by 
ğ‘¥
x.

The Poisson correction 
ğœ“
ğ‘˜
Ïˆ
k
	â€‹

 absorbs that variation so that the corrected process behaves like a constantâ€‘drift walk (plus bounded noise).

Once you have this, you can invoke finiteâ€‘state Fosterâ€“Lyapunov results to get:

descent to a compact set with high probability / almost surely (under the probabilistic model),

quantitative bounds on return times, tail probabilities, etc.

3) What M4 evidence supports it

ğ‘”
ğ‘˜
(
ğ‘¥
)
g
k
	â€‹

(x) is negative on average and not â€œnearly zeroâ€ (you want a margin).

Solving the empirical Poisson equation using 
ğ‘ƒ
^
ğ‘˜
,
ğ‘”
^
ğ‘˜
P
k
	â€‹

,
g
	â€‹

k
	â€‹

 yields a 
ğœ“
^
ğ‘˜
Ïˆ
	â€‹

k
	â€‹

 such that the corrected drift

ğ‘‘
^
ğ‘˜
(
ğ‘¥
)
:
=
ğ‘”
^
ğ‘˜
(
ğ‘¥
)
+
(
ğ‘ƒ
^
ğ‘˜
ğœ“
^
ğ‘˜
)
(
ğ‘¥
)
âˆ’
ğœ“
^
ğ‘˜
(
ğ‘¥
)
d
k
	â€‹

(x):=
g
	â€‹

k
	â€‹

(x)+(
P
k
	â€‹

Ïˆ
	â€‹

k
	â€‹

)(x)âˆ’
Ïˆ
	â€‹

k
	â€‹

(x)

is nearly constant across 
ğ‘¥
x, and that constant is negative with margin.

The worst corrected drift state isnâ€™t scary (or if it is, it tells you exactly what extra state info you need).

4) Plausible proof strategy

This is very â€œhumanâ€‘doableâ€ once Lemma 1 is in place:

On a finite irreducible aperiodic chain, the Poisson equation has a bounded solution (unique up to an additive constant) as soon as you pin down 
ğœ‹
ğ‘˜
(
ğœ“
ğ‘˜
)
=
0
Ï€
k
	â€‹

(Ïˆ
k
	â€‹

)=0.

The remaining work is to lift the statement from the finite chain approximation to the actual Syracuse dynamics under your evolved measure, and to manage the 
log
â¡
(
3
+
1
/
ğ‘›
)
log(3+1/n) vs 
log
â¡
3
log3 correction by taking 
ğ‘
0
N
0
	â€‹

 large and bruteâ€‘forcing below 
ğ‘
0
N
0
	â€‹

.

5) Minimal extra experiments to deâ€‘risk it

Compute 
ğœ“
^
ğ‘˜
Ïˆ
	â€‹

k
	â€‹

 for 
ğ‘˜
=
8
,
10
,
12
k=8,10,12 and compare: does 
ğœ“
^
ğ‘˜
Ïˆ
	â€‹

k
	â€‹

 stabilize (modulo additive constant) as 
ğ‘˜
k grows?

Measure the empirical distribution of the corrected increment

ğ‘
ğ‘¡
:
=
Î”
ğ‘¡
+
ğœ“
ğ‘˜
(
ğ‘‹
ğ‘¡
+
1
)
âˆ’
ğœ“
ğ‘˜
(
ğ‘‹
ğ‘¡
)
Z
t
	â€‹

:=Î”
t
	â€‹

+Ïˆ
k
	â€‹

(X
t+1
	â€‹

)âˆ’Ïˆ
k
	â€‹

(X
t
	â€‹

)

and record:

mean,

variance,

worstâ€‘state conditional mean,

tail behavior.

If the â€œworstâ€‘state conditional meanâ€ is near 0 or positive, youâ€™ve learned something extremely valuable: your state space is missing a crucial bit (maybe 
ğ‘‹
ğ‘¡
â€Š
m
o
d
â€Š
3
ğ‘˜
+
1
X
t
	â€‹

mod3
k+1
, or parity of 
ğ‘
ğ‘¡
a
t
	â€‹

, etc.).

Lemma candidate 4 â€” Exponential rarity of â€œbad blocksâ€ (large deviations)
1) Precise statement

Fix 
ğ‘˜
k and use the corrected increment 
ğ‘
ğ‘¡
Z
t
	â€‹

 from Lemma 3. Let 
ğ‘†
ğ¿
:
=
âˆ‘
ğ‘¡
=
0
ğ¿
âˆ’
1
ğ‘
ğ‘¡
S
L
	â€‹

:=âˆ‘
t=0
Lâˆ’1
	â€‹

Z
t
	â€‹

.

 

Lemma target (uniform block contraction).
There exist 
ğ¿
âˆˆ
ğ‘
LâˆˆN and constants 
ğœ‚
>
0
Î·>0, 
ğ‘
>
0
b>0 such that uniformly over starting states 
ğ‘¥
âˆˆ
ğ‘†
ğ‘˜
xâˆˆS
k
	â€‹

,

ğ‘ƒ
(
ğ‘†
ğ¿
â‰¥
âˆ’
ğ‘
ğ¿
âˆ£
ğ‘‹
0
=
ğ‘¥
)
â‰¤
ğ‘’
âˆ’
ğœ‚
ğ¿
.
P(S
L
	â€‹

â‰¥âˆ’bLâˆ£X
0
	â€‹

=x)â‰¤e
âˆ’Î·L
.

A â€œnicerâ€ special case is 
ğ‘
=
0
b=0:

ğ‘ƒ
(
ğ‘†
ğ¿
â‰¥
0
âˆ£
ğ‘‹
0
=
ğ‘¥
)
â‰¤
ğ‘’
âˆ’
ğœ‚
ğ¿
.
P(S
L
	â€‹

â‰¥0âˆ£X
0
	â€‹

=x)â‰¤e
âˆ’Î·L
.
2) Why this matters

This is the lever that turns â€œnegative drift in expectationâ€ into â€œalmost sure descentâ€:

If blocks with nonâ€‘negative (or insufficiently negative) drift are exponentially rare, you can unionâ€‘bound/Borelâ€“Cantelli your way to: only finitely many bad blocks occur almost surely.

Then 
ğ‘‰
ğ‘˜
(
ğ‘›
ğ‘¡
)
V
k
	â€‹

(n
t
	â€‹

) tends to 
âˆ’
âˆ
âˆ’âˆ, meaning 
ğ‘›
ğ‘¡
n
t
	â€‹

 enters a bounded region.

This is exactly where M3 (autocorrelation) and M2 (mixing) cash out into a theoremâ€‘shaped inequality.

3) What M2â€“M4 evidence supports it

From M4: empirical block sums 
ğ‘†
ğ¿
S
L
	â€‹

 have mean 
â‰ˆ
âˆ’
ğ‘
ğ¿
â‰ˆâˆ’cL with 
ğ‘
>
0
c>0, and the upper tail 
ğ‘ƒ
(
ğ‘†
ğ¿
â‰¥
0
)
P(S
L
	â€‹

â‰¥0) drops roughly like 
exp
â¡
(
âˆ’
ğœ‚
ğ¿
)
exp(âˆ’Î·L).

From M3: correlations decay fast enough that the block sums behave close to a Markovâ€‘additive process (not a longâ€‘memory monster).

From M2: your modâ€‘
3
ğ‘˜
3
k
 state doesnâ€™t lock into slow â€œbadâ€ modes; otherwise the tail wonâ€™t be exponential uniformly in starting state.

4) Plausible proof strategy

For a finiteâ€‘state Markov chain, this is wonderfully standard and explicit:

Consider the tilted matrix for 
ğœ†
>
0
Î»>0:

ğ‘€
ğœ†
(
ğ‘¥
,
ğ‘¦
)
:
=
ğ‘ƒ
ğ‘˜
(
ğ‘¥
,
ğ‘¦
)
â€‰
ğ¸
â€‰â£
[
ğ‘’
ğœ†
ğ‘
ğ‘¡
âˆ£
ğ‘‹
ğ‘¡
=
ğ‘¥
,
ğ‘‹
ğ‘¡
+
1
=
ğ‘¦
]
.
M
Î»
	â€‹

(x,y):=P
k
	â€‹

(x,y)E[e
Î»Z
t
	â€‹

âˆ£X
t
	â€‹

=x,X
t+1
	â€‹

=y].

The exponential growth rate of 
ğ¸
(
ğ‘’
ğœ†
ğ‘†
ğ¿
)
E(e
Î»S
L
	â€‹

) is controlled by the Perronâ€“Frobenius eigenvalue 
ğœŒ
(
ğ‘€
ğœ†
)
Ï(M
Î»
	â€‹

).

If the mean drift is negative, you can show there exists 
ğœ†
>
0
Î»>0 with 
ğœŒ
(
ğ‘€
ğœ†
)
<
1
Ï(M
Î»
	â€‹

)<1, giving a Chernoff bound:

ğ‘ƒ
(
ğ‘†
ğ¿
â‰¥
0
)
â‰¤
ğ¸
(
ğ‘’
ğœ†
ğ‘†
ğ¿
)
â‰¤
ğ¶
â€‰
ğœŒ
(
ğ‘€
ğœ†
)
ğ¿
.
P(S
L
	â€‹

â‰¥0)â‰¤E(e
Î»S
L
	â€‹

)â‰¤CÏ(M
Î»
	â€‹

)
L
.

All of this is finiteâ€‘dimensional linear algebra once you have a validated 
ğ‘ƒ
ğ‘˜
P
k
	â€‹

 and a validated conditional law of 
ğ‘
ğ‘¡
Z
t
	â€‹

.

5) Minimal extra experiments to deâ€‘risk it

Estimate the empirical mgf 
ğ¸
^
(
ğ‘’
ğœ†
ğ‘†
ğ¿
)
E
(e
Î»S
L
	â€‹

) for a grid of 
ğœ†
Î» and 
ğ¿
L; see if 
log
â¡
ğ¸
^
(
ğ‘’
ğœ†
ğ‘†
ğ¿
)
/
ğ¿
log
E
(e
Î»S
L
	â€‹

)/L stabilizes.

Identify the dominant contribution to bad blocks: is it many 
ğ‘
=
1
a=1 in a row, or some stateâ€‘dependent pattern? That will guide whether you need to refine the state space.

Uniformity check: estimate 
ğ‘ƒ
(
ğ‘†
ğ¿
â‰¥
0
âˆ£
ğ‘‹
0
=
ğ‘¥
)
P(S
L
	â€‹

â‰¥0âˆ£X
0
	â€‹

=x) for a sample of â€œworstâ€ 
ğ‘¥
xâ€™s.

Lemma candidate 5 â€” Conditional â€œalmost sure terminationâ€ from block descent + finite verification

This is the optional â€œcapstone lemmaâ€ that turns the previous lemmas into a result that looks like progress toward termination.

1) Precise statement (conditional theorem)

Assume Lemmas 1â€“4 hold for some 
ğ‘˜
k with explicit constants, and assume further that all odd 
ğ‘›
â‰¤
ğ‘
â‹†
nâ‰¤N
â‹†
	â€‹

 are verified to reach 
1
1 under 
ğ‘‡
T.

 

Then for 
ğ‘›
0
n
0
	â€‹

 drawn from your evolved measure (e.g., logâ€‘uniform on a large range, evolved past burnâ€‘in), the Syracuse trajectory hits 
[
1
,
ğ‘
â‹†
]
[1,N
â‹†
	â€‹

] almost surely; hence it reaches 
1
1 almost surely (for that measure).

 

You can state it in density language: the set of starting values failing to reach 
1
1 has logarithmic density 0.

 

This would be in the spirit of (but stronger in conclusion than) Taoâ€™s logarithmicâ€‘density â€œalmost boundedâ€ theorem. 
arXiv

2) Why this matters

Itâ€™s a clean â€œproofâ€‘shapedâ€ deliverable:

youâ€™re not proving Collatz for all 
ğ‘›
n,

but youâ€™re converting empirical drift/mixing into a rigorous â€œalmost allâ€ termination statement with explicit constants and an explicit finite verification threshold.

Thatâ€™s exactly the kind of result humans can plausibly prove in the near term.

3) What evidence supports it

Your block contraction probability is strong enough that you can bound the chance of avoiding 
[
1
,
ğ‘
â‹†
]
[1,N
â‹†
	â€‹

] across many blocks by a summable series.

Empirically, the escape probability seems to fall faster than any power.

4) Plausible proof strategy

Use Lemma 3 to build a supermartingale 
ğ‘‰
ğ‘˜
(
ğ‘›
ğ‘¡
)
V
k
	â€‹

(n
t
	â€‹

).

Use Lemma 4 to get an exponential tail for excursions upward / for failure to decrease over blocks.

Apply a stopping time argument to show 
ğ‘‰
ğ‘˜
V
k
	â€‹

 must hit the finite region almost surely.

Patch with brute force for 
ğ‘›
â‰¤
ğ‘
â‹†
nâ‰¤N
â‹†
	â€‹

.

5) Minimal extra experiments to deâ€‘risk it

Choose a realistic 
ğ‘
â‹†
N
â‹†
	â€‹

 you can fully verify (maybe you already have this infrastructure).

Empirically estimate the probability of avoiding 
[
1
,
ğ‘
â‹†
]
[1,N
â‹†
	â€‹

] up to time 
ğ‘¡
t and check that itâ€™s consistent with an exponential bound derived from Lemma 4â€™s 
ğœ‚
Î·.

Parameter recommendations (k, horizons, samples)

You want parameters that (i) make the finiteâ€‘state objects tractable, (ii) are large enough to be convincing, (iii) let you see stability as 
ğ‘˜
k grows.

Suggested 
ğ‘˜
k ladder

Start: 
ğ‘˜
=
8
k=8.
State space size 
âˆ£
ğ‘†
ğ‘˜
âˆ£
=
2
â‹…
3
ğ‘˜
âˆ’
1
=
2
â‹…
3
7
=
4374
âˆ£S
k
	â€‹

âˆ£=2â‹…3
kâˆ’1
=2â‹…3
7
=4374. Thatâ€™s small enough to:

build 
ğ‘ƒ
^
ğ‘˜
P
k
	â€‹

 accurately,

solve Poisson equations comfortably,

do eigen/spectral diagnostics.

Confirm: 
ğ‘˜
=
10
k=10.

âˆ£
ğ‘†
10
âˆ£
=
2
â‹…
3
9
=
39366
âˆ£S
10
	â€‹

âˆ£=2â‹…3
9
=39366. This is the first â€œseriousâ€ size where spurious finiteâ€‘state artifacts start dying.

Stress test: 
ğ‘˜
=
12
k=12.

âˆ£
ğ‘†
12
âˆ£
=
2
â‹…
3
11
=
354294
âˆ£S
12
	â€‹

âˆ£=2â‹…3
11
=354294. You probably wonâ€™t do dense linear algebra, but you can still do:

sparse power iteration for leading eigenvalues,

Monteâ€‘Carlo estimates of block tails,

approximate Poisson solves (iterative methods).

Horizons

Mixing (M2): measure up to 
ğ‘¡
=
50
t=50 for 
ğ‘˜
=
8
k=8, 
ğ‘¡
=
100
t=100â€“
200
200 for 
ğ‘˜
=
10
k=10.
The decisive thing is seeing a clear exponential regime, not â€œwiggles.â€

Autocorrelation (M3): compute ACF / mutual information out to lag 50â€“100.

Block tails (Lemma 4 / M4â€‘adjacent): block lengths 
ğ¿
âˆˆ
{
20
,
50
,
100
,
200
}
Lâˆˆ{20,50,100,200}.
Youâ€™re hunting an exponential tail; those 
ğ¿
Lâ€™s usually reveal it.

Sample sizes (rule of thumb)

Think in stateâ€‘visits, not trajectories.

For 
ğ‘˜
=
8
k=8: target 
4
Ã—
10
7
4Ã—10
7
 stateâ€‘visits.
That gives ~9k visits per state if nearâ€‘uniform.

For 
ğ‘˜
=
10
k=10: target 
1
Ã—
10
8
1Ã—10
8
 stateâ€‘visits.
That gives ~2500 visits per state on averageâ€”enough to stabilize conditional means.

You can get these visits by, e.g., 
2
Ã—
10
5
2Ã—10
5
 trajectories of length 200 (40M visits), then scale.

Diagnostics that are most decisive

If you only had time to compute a handful of plots/numbers, these are the â€œproofâ€‘shapedâ€ ones:

Spectral gap proxy for 
ğ‘ƒ
ğ‘˜
P
k
	â€‹

: estimate 
âˆ£
ğœ†
2
âˆ£
âˆ£Î»
2
	â€‹

âˆ£ (or mixing time via TV distance decay).
If 
âˆ£
ğœ†
2
âˆ£
âˆ£Î»
2
	â€‹

âˆ£ creeps toward 1 as 
ğ‘˜
k increases, your state is missing something.

Worstâ€‘state corrected drift: after computing 
ğœ“
ğ‘˜
Ïˆ
k
	â€‹

,

min
â¡
ğ‘¥
Â 
ğ¸
^
[
ğ‘
ğ‘¡
âˆ£
ğ‘‹
ğ‘¡
=
ğ‘¥
]
x
min
	â€‹

Â 
E
[Z
t
	â€‹

âˆ£X
t
	â€‹

=x]

(and maybe also a small quantile like 1%).
Proofs die in worstâ€‘case corners.

Block nonâ€‘descent probability vs 
ğ¿
L:

ğ‘
(
ğ¿
)
:
=
sup
â¡
ğ‘¥
ğ‘ƒ
^
(
ğ‘†
ğ¿
â‰¥
0
âˆ£
ğ‘‹
0
=
ğ‘¥
)
.
p(L):=
x
sup
	â€‹

P
(S
L
	â€‹

â‰¥0âˆ£X
0
	â€‹

=x).

Plot 
log
â¡
ğ‘
(
ğ¿
)
logp(L) vs 
ğ¿
L. A straight line is the smell of a future Chernoff bound.

Conditional deviation from geometric for 
ğ‘
a: the distributional sanity check that prevents selfâ€‘deception:

sup
â¡
ğ‘¥
ğ·
K
L
â€‰â£
(
ğ‘ƒ
^
(
ğ‘
âˆ£
ğ‘‹
=
ğ‘¥
)
â€‰
âˆ¥
â€‰
Geom
(
1
/
2
)
)
.
x
sup
	â€‹

D
KL
	â€‹

(
P
(aâˆ£X=x)âˆ¥Geom(1/2)).

If a few states are wildly off, you know exactly where to refine the state.

Closing thought

The â€œdream stackâ€ is: (Markov/spectral gap) â‡’ (Poisson correction) â‡’ (large deviations) â‡’ (almost sure descent to a finite verified set). Thatâ€™s a very sane proof pipeline: finiteâ€‘dimensional linear algebra + explicit constants + a controlled exceptional set. It also has a nice feature: every time an empirical diagnostic fails, it doesnâ€™t just say â€œoopsâ€â€”it tells you what information your state is missing.

 

Your M2â€“M4 plan is basically a truthâ€‘serum test for whether this pipeline is viable.

 

(And yes, the universe is ridiculous: a conjecture about dividing by 2 and multiplying by 3 pushes you into spectral gaps on 
3
3-adic groups. Mathematics is a prankster.)
