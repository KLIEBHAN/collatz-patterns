# M2â€“M4 Experimental Specification (GPT 5.2 Pro extract)

Source: ChatGPT conversation `WEB:52812852-4763-42ea-a69e-be6c9c59a313` (extracted via `chatgpt-extract.sh`).

> Note: The extract contains unicode/math formatting artifacts; keep as-is for provenance.

---

M2 â€” Mixing / bias of accelerated odd map mod 
3
ğ‘˜
3
k
Hypotheses

H2.1 (mixing): For fixed 
ğ‘˜
k, the empirical distribution 
ğœ‡
ğ‘˜
,
ğ‘¡
Î¼
k,t
	â€‹

 of 
ğ‘‡
ğ‘¡
(
ğ‘›
)
â€Š
m
o
d
â€Š
3
ğ‘˜
T
t
(n)mod3
k
 (for odd 
ğ‘›
âˆ¼
Unif
{
1..
ğ‘
}
nâˆ¼Unif{1..N}) approaches a time-stable distribution 
ğœ‹
ğ‘˜
Ï€
k
	â€‹

 as 
ğ‘¡
t grows (after a short burn-in).

H2.2 (bias structure): 
ğœ‹
ğ‘˜
Ï€
k
	â€‹

 is not uniform on 
(
ğ‘
/
3
ğ‘˜
ğ‘
)
Ã—
(Z/3
k
Z)
Ã—
; deviations are stable across 
ğ‘
N and reflect intrinsic Syracuse bias (not a sampling artifact).

Metrics

Let 
ğ‘ˆ
ğ‘˜
=
(
ğ‘
/
3
ğ‘˜
ğ‘
)
Ã—
U
k
	â€‹

=(Z/3
k
Z)
Ã—
 (size 
âˆ£
ğ‘ˆ
ğ‘˜
âˆ£
=
2
â‹…
3
ğ‘˜
âˆ’
1
âˆ£U
k
	â€‹

âˆ£=2â‹…3
kâˆ’1
). Define empirical histogram:

ğœ‡
ğ‘˜
,
ğ‘¡
(
ğ‘Ÿ
)
=
Pr
â¡
[
ğ‘‡
ğ‘¡
(
ğ‘›
)
â‰¡
ğ‘Ÿ
Â 
(
modÂ 
3
ğ‘˜
)
]
,
ğ‘Ÿ
âˆˆ
ğ‘ˆ
ğ‘˜
.
Î¼
k,t
	â€‹

(r)=Pr[T
t
(n)â‰¡rÂ (modÂ 3
k
)],râˆˆU
k
	â€‹

.

Convergence / stability:

TV distance: 
ğ‘‘
TV
(
ğœ‡
,
ğœˆ
)
=
1
2
âˆ‘
ğ‘Ÿ
âˆˆ
ğ‘ˆ
ğ‘˜
âˆ£
ğœ‡
(
ğ‘Ÿ
)
âˆ’
ğœˆ
(
ğ‘Ÿ
)
âˆ£
d
TV
	â€‹

(Î¼,Î½)=
2
1
	â€‹

âˆ‘
râˆˆU
k
	â€‹

	â€‹

âˆ£Î¼(r)âˆ’Î½(r)âˆ£.

KL divergence: 
ğ·
KL
(
ğœ‡
âˆ¥
ğœˆ
)
=
âˆ‘
ğ‘Ÿ
ğœ‡
(
ğ‘Ÿ
)
log
â¡
ğœ‡
(
ğ‘Ÿ
)
ğœˆ
(
ğ‘Ÿ
)
D
KL
	â€‹

(Î¼âˆ¥Î½)=âˆ‘
r
	â€‹

Î¼(r)log
Î½(r)
Î¼(r)
	â€‹

 (use add-
ğœ–
Ïµ smoothing).

Chi-square: 
ğœ’
2
(
ğœ‡
,
ğœˆ
)
=
âˆ‘
ğ‘Ÿ
(
ğœ‡
(
ğ‘Ÿ
)
âˆ’
ğœˆ
(
ğ‘Ÿ
)
)
2
ğœˆ
(
ğ‘Ÿ
)
Ï‡
2
(Î¼,Î½)=âˆ‘
r
	â€‹

Î½(r)
(Î¼(r)âˆ’Î½(r))
2
	â€‹

 (requires 
ğœˆ
(
ğ‘Ÿ
)
>
0
Î½(r)>0).

Operational convergence: for a fixed â€œreference timeâ€ 
ğ‘¡
â‹†
t
â‹†
	â€‹

 (late), measure 
ğ‘‘
TV
(
ğœ‡
ğ‘˜
,
ğ‘¡
,
ğœ‡
ğ‘˜
,
ğ‘¡
â‹†
)
d
TV
	â€‹

(Î¼
k,t
	â€‹

,Î¼
k,t
â‹†
	â€‹

	â€‹

) for 
ğ‘¡
<
ğ‘¡
â‹†
t<t
â‹†
	â€‹

.

Design

Sample odd 
ğ‘›
n uniformly from 
[
1
,
ğ‘
]
[1,N]. Track orbit 
ğ‘›
ğ‘–
=
ğ‘‡
ğ‘–
(
ğ‘›
)
n
i
	â€‹

=T
i
(n) for 
ğ‘–
=
0..
ğ‘¡
max
â¡
i=0..t
max
	â€‹

.

At each 
ğ‘¡
t, record 
ğ‘Ÿ
ğ‘˜
,
ğ‘¡
=
ğ‘›
ğ‘¡
â€Š
m
o
d
â€Š
3
ğ‘˜
r
k,t
	â€‹

=n
t
	â€‹

mod3
k
 only if 
ğ‘›
ğ‘¡
â‰¡Ì¸
0
(
m
o
d
3
)
n
t
	â€‹

î€ 
â‰¡0(mod3) (expected after first step, but still enforce).

Compute 
ğœ‡
ğ‘˜
,
ğ‘¡
Î¼
k,t
	â€‹

 for each 
ğ‘˜
,
ğ‘¡
k,t.

Holdout check: repeat on disjoint ranges (e.g., 
[
1
,
ğ‘
/
2
]
[1,N/2] vs 
(
ğ‘
/
2
,
ğ‘
]
(N/2,N]) or two independent hash-samples; compare resulting 
ğœ‡
ğ‘˜
,
ğ‘¡
Î¼
k,t
	â€‹

.

Parameters

Suggested 
ğ‘˜
âˆˆ
{
1
,
â€¦
,
8
}
kâˆˆ{1,â€¦,8} (optionally 9â€“10 if counts per bin stay high).

ğ‘¡
âˆˆ
{
0
,
â€¦
,
60
}
tâˆˆ{0,â€¦,60} with focus on 
ğ‘¡
â‰¤
40
tâ‰¤40 for mixing curves; pick 
ğ‘¡
â‹†
=
50
t
â‹†
	â€‹

=50 as â€œlateâ€ reference.

Sample size: target 
â‰¥
200
â‹…
âˆ£
ğ‘ˆ
ğ‘˜
âˆ£
â‰¥200â‹…âˆ£U
k
	â€‹

âˆ£ samples per 
(
ğ‘˜
,
ğ‘¡
)
(k,t) bin for stable chi-square; for 
ğ‘˜
=
8
k=8, 
âˆ£
ğ‘ˆ
ğ‘˜
âˆ£
=
2
â‹…
3
7
=
4374
âˆ£U
k
	â€‹

âˆ£=2â‹…3
7
=4374, so 
âˆ¼
10
6
âˆ¼10
6
 samples is ample. Full sweep up to 50M odds is overkill; hashed subsample 
ğ‘†
âˆ¼
1
â€“
5
Sâˆ¼1â€“5M is fine.

Interpretation

Rapid decay of 
ğ‘‘
TV
(
ğœ‡
ğ‘˜
,
ğ‘¡
,
ğœ‡
ğ‘˜
,
ğ‘¡
â‹†
)
d
TV
	â€‹

(Î¼
k,t
	â€‹

,Î¼
k,t
â‹†
	â€‹

	â€‹

) with 
ğ‘¡
t supports a â€œnear-stationary residue biasâ€ usable in drift modeling.

Stable non-uniform 
ğœ‹
ğ‘˜
Ï€
k
	â€‹

 across 
ğ‘
N indicates intrinsic bias (candidate for residue-corrected potentials).

Failure modes

Conditioning artifacts: sampling only odds is correct for odd-map dynamics but must be consistent across 
ğ‘¡
t. Donâ€™t mix â€œall integersâ€ with â€œodds only.â€

Multiplicity-of-3 handling: since 
ğ‘‡
T is defined on odds and outputs odds, mod 
3
ğ‘˜
3
k
 state space should be restricted to units; accidentally including residue 0 corrupts distances.

Small-
ğ‘›
n attractor bias: late-time samples overrepresent small values because many trajectories fall; mitigate by (i) restricting analysis to early 
ğ‘¡
t, (ii) reporting results with and without excluding samples where 
ğ‘›
ğ‘¡
â‰¤
ğ‘›
min
â¡
n
t
	â€‹

â‰¤n
min
	â€‹

 (e.g., 
ğ‘›
min
â¡
=
10
6
n
min
	â€‹

=10
6
).

Finite-
ğ‘
N edge effects: validate stability across disjoint 
ğ‘›
n-ranges.

M3 â€” Distribution & autocorrelation of 
ğ‘
(
ğ‘›
)
=
ğ‘£
2
(
3
ğ‘›
+
1
)
a(n)=v
2
	â€‹

(3n+1) under evolved measure
Hypotheses

H3.1: Marginal 
ğ‘
ğ‘¡
:
=
ğ‘
(
ğ‘›
ğ‘¡
)
a
t
	â€‹

:=a(n
t
	â€‹

) is close to geometric on 
{
1
,
2
,
â€¦
â€‰
}
{1,2,â€¦} under the evolved measure at moderate 
ğ‘¡
t (possibly with residue-dependent deviations).

H3.2: Autocorrelation of 
(
ğ‘
ğ‘¡
)
(a
t
	â€‹

) decays quickly (short memory), possibly faster after conditioning on residue 
ğ‘›
ğ‘¡
â€Š
m
o
d
â€Š
3
ğ‘˜
n
t
	â€‹

mod3
k
.

Metrics

Histograms 
ğ‘
ğ‘¡
(
ğ‘—
)
=
Pr
â¡
[
ğ‘
ğ‘¡
=
ğ‘—
]
p
t
	â€‹

(j)=Pr[a
t
	â€‹

=j], for 
ğ‘—
=
1..
ğ½
j=1..J (cap 
ğ½
âˆ¼
30
Jâˆ¼30, aggregate tail).

Fit to geometric 
ğ‘
ğœƒ
(
ğ‘—
)
=
(
1
âˆ’
ğœƒ
)
ğ‘—
âˆ’
1
ğœƒ
q
Î¸
	â€‹

(j)=(1âˆ’Î¸)
jâˆ’1
Î¸; compare via TV/KL, and report MLE 
ğœƒ
^
ğ‘¡
Î¸
^
t
	â€‹

.

Conditional histograms 
ğ‘
ğ‘¡
,
ğ‘Ÿ
(
ğ‘—
)
=
Pr
â¡
[
ğ‘
ğ‘¡
=
ğ‘—
âˆ£
ğ‘›
ğ‘¡
â€Š
m
o
d
â€Š
3
ğ‘˜
=
ğ‘Ÿ
]
p
t,r
	â€‹

(j)=Pr[a
t
	â€‹

=jâˆ£n
t
	â€‹

mod3
k
=r].

Autocorrelation: 
ğœŒ
(
â„“
)
=
C
o
r
r
(
ğ‘
ğ‘¡
,
ğ‘
ğ‘¡
+
â„“
)
Ï(â„“)=Corr(a
t
	â€‹

,a
t+â„“
	â€‹

) (pooled over 
ğ‘¡
t in a window), plus mutual information 
ğ¼
(
ğ‘
ğ‘¡
;
ğ‘
ğ‘¡
+
â„“
)
I(a
t
	â€‹

;a
t+â„“
	â€‹

) as a non-linear dependence check.

Design

Choose a â€œstationary-ishâ€ time window 
ğ‘¡
âˆˆ
[
ğ‘¡
0
,
ğ‘¡
1
]
tâˆˆ[t
0
	â€‹

,t
1
	â€‹

] (from M2 mixing curves), e.g. 
ğ‘¡
0
=
15
,
ğ‘¡
1
=
35
t
0
	â€‹

=15,t
1
	â€‹

=35.

For sampled trajectories, record 
ğ‘
ğ‘¡
a
t
	â€‹

 for 
ğ‘¡
âˆˆ
[
ğ‘¡
0
,
ğ‘¡
1
]
tâˆˆ[t
0
	â€‹

,t
1
	â€‹

] and residues 
ğ‘Ÿ
=
ğ‘›
ğ‘¡
â€Š
m
o
d
â€Š
3
ğ‘˜
r=n
t
	â€‹

mod3
k
.

Compute global 
ğ‘
(
ğ‘—
)
p(j) pooled over 
ğ‘¡
âˆˆ
[
ğ‘¡
0
,
ğ‘¡
1
]
tâˆˆ[t
0
	â€‹

,t
1
	â€‹

], and conditional 
ğ‘
ğ‘Ÿ
(
ğ‘—
)
p
r
	â€‹

(j) for each residue (or coarse bins if data sparse).

Compute 
ğœŒ
(
â„“
)
Ï(â„“) for 
â„“
=
1..
ğ¿
â„“=1..L (e.g., 
ğ¿
=
20
L=20), both unconditional and conditional on residue classes (or by residualizing: subtract 
ğ¸
[
ğ‘
âˆ£
ğ‘Ÿ
]
E[aâˆ£r]).

Parameters

ğ‘˜
âˆˆ
{
4
,
6
,
8
}
kâˆˆ{4,6,8} (tradeoff between resolution and per-state counts).

ğ½
=
30
J=30, tail bin 
ğ‘—
>
ğ½
j>J.

Sample size: 
â‰¥
10
6
â‰¥10
6
 state-visits in window; for conditional on 
ğ‘˜
=
8
k=8, consider aggregating residues by 
ğ‘Ÿ
â€Š
m
o
d
â€Š
3
ğ‘˜
â€²
rmod3
k
â€²
 with 
ğ‘˜
â€²
<
ğ‘˜
k
â€²
<k if sparse.

Interpretation

Close-to-geometric 
ğ‘
(
ğ‘—
)
p(j) supports drift heuristics; systematic residue-dependent deviations identify â€œdanger residues.â€

Correlation length estimate: smallest 
â„“
â„“ with 
âˆ£
ğœŒ
(
â„“
)
âˆ£
<
ğœ
âˆ£Ï(â„“)âˆ£<Ï„ (e.g., 
ğœ
=
0.02
Ï„=0.02) consistently across holdouts.

Failure modes

Evolved-measure bias: pooling over 
ğ‘¡
t too late overweights small 
ğ‘›
n; stick to mixing window, and report sensitivity to 
ğ‘¡
0
,
ğ‘¡
1
t
0
	â€‹

,t
1
	â€‹

.

Tail noise: large 
ğ‘—
j rare; always aggregate tail and report confidence intervals (bootstrap over trajectories).

M4 â€” Drift measurement: 
ğ¸
[
Î”
log
â¡
ğ‘›
]
E[Î”logn] and state-dependent drift
Hypotheses

H4.1: Mean drift 
ğ¸
[
Î”
ğ‘¡
]
E[Î”
t
	â€‹

] is negative in the mixing window, where 
Î”
ğ‘¡
=
log
â¡
ğ‘›
ğ‘¡
+
1
âˆ’
log
â¡
ğ‘›
ğ‘¡
Î”
t
	â€‹

=logn
t+1
	â€‹

âˆ’logn
t
	â€‹

.

H4.2: Drift depends on residue 
ğ‘Ÿ
=
ğ‘›
ğ‘¡
â€Š
m
o
d
â€Š
3
ğ‘˜
r=n
t
	â€‹

mod3
k
; a small set of residues have less-negative or positive drift (â€œbad statesâ€).

Metrics

Î”
ğ‘¡
=
log
â¡
(
ğ‘›
ğ‘¡
+
1
)
âˆ’
log
â¡
(
ğ‘›
ğ‘¡
)
Î”
t
	â€‹

=log(n
t+1
	â€‹

)âˆ’log(n
t
	â€‹

) (natural log).

Global drift 
Î”
Ë‰
=
ğ¸
[
Î”
ğ‘¡
]
Î”
Ë‰
=E[Î”
t
	â€‹

] pooled over 
ğ‘¡
âˆˆ
[
ğ‘¡
0
,
ğ‘¡
1
]
tâˆˆ[t
0
	â€‹

,t
1
	â€‹

].

State drift 
ğ‘”
(
ğ‘Ÿ
)
=
ğ¸
[
Î”
ğ‘¡
âˆ£
ğ‘›
ğ‘¡
â€Š
m
o
d
â€Š
3
ğ‘˜
=
ğ‘Ÿ
]
g(r)=E[Î”
t
	â€‹

âˆ£n
t
	â€‹

mod3
k
=r].

â€œBadnessâ€ score: 
ğ‘
(
ğ‘Ÿ
)
=
ğ‘”
(
ğ‘Ÿ
)
âˆ’
m
e
d
i
a
n
ğ‘Ÿ
â€²
ğ‘”
(
ğ‘Ÿ
â€²
)
b(r)=g(r)âˆ’median
r
â€²
	â€‹

g(r
â€²
) or simply rank by 
ğ‘”
(
ğ‘Ÿ
)
g(r).

Uncertainty: standard errors per state; require minimum count per state (e.g., 
â‰¥
5000
â‰¥5000).

Design

Use same time window 
[
ğ‘¡
0
,
ğ‘¡
1
]
[t
0
	â€‹

,t
1
	â€‹

] as M3.

For each state visit, compute 
Î”
ğ‘¡
Î”
t
	â€‹

 and accumulate sums/counts globally and per residue.

Identify bad states as those with:

ğ‘”
(
ğ‘Ÿ
)
â‰¥
0
g(r)â‰¥0 (strong criterion), or

top 
1
%
1% by 
ğ‘”
(
ğ‘Ÿ
)
g(r) with CI excluding global mean.

Parameters

ğ‘˜
âˆˆ
{
6
,
8
}
kâˆˆ{6,8}; 
ğ‘¡
0
,
ğ‘¡
1
t
0
	â€‹

,t
1
	â€‹

 from M2.

Optional robustness: compute drift for block steps 
Î”
(
ğ‘š
)
=
log
â¡
ğ‘›
ğ‘¡
+
ğ‘š
âˆ’
log
â¡
ğ‘›
ğ‘¡
Î”
(m)
=logn
t+m
	â€‹

âˆ’logn
t
	â€‹

 for 
ğ‘š
âˆˆ
{
5
,
10
,
20
}
mâˆˆ{5,10,20}.

Interpretation

Uniform negativity of 
ğ‘”
(
ğ‘Ÿ
)
g(r) (for some 
ğ‘˜
k) is a concrete â€œfinite-state drift certificateâ€ candidate (still conjectural).

If only few bad states exist, focus later work on proving or forcing escape from them (candidate reduction).

Failure modes

Survivorship bias: trajectories that descend quickly contribute more late-time samples at small 
ğ‘›
n; limit to mixing window and report sensitivity.

State sparsity: for larger 
ğ‘˜
k, many residues under-sampled; enforce count thresholds and/or reduce 
ğ‘˜
k.

Repo documentation (minimal, reproducible)

experiments/M2_mixing/

config.yaml (N, sample method, k list, t range, t_star)

counts_mu_k_t.npz (histograms)

mixing_metrics.csv (TV/KL/chi2 vs t)

report.md (plots + narrative)

experiments/M3_a_dist/

hist_a_global.csv, hist_a_by_residue_k{K}.parquet

autocorr_a.csv, fits_geometric.csv

report.md

experiments/M4_drift/

drift_global.csv, drift_by_residue_k{K}.parquet

bad_states_k{K}.csv (with counts, CIs)

Naming: N{N}_S{S}_k{K}_t{t0}-{t1}_seed{seed}_git{sha} embedded in filenames; all plots generated by scripts that read only these artifacts.
